{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "By: Rashaad Ratliff-Brown and Abhijith Tammanagari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#import spacy\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from time import time \n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blurbs</th>\n",
       "      <th>Backed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Catalysts, Explorers &amp; Secret Keepers: Women ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A unique handmade picture book for kids &amp; art ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A horror comedy about a repairman who was in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Johnny Wander autobio omnibus you've all b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The vision for this project is the establishme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Blurbs  Backed\n",
       "0  'Catalysts, Explorers & Secret Keepers: Women ...       1\n",
       "1  A unique handmade picture book for kids & art ...       0\n",
       "2  A horror comedy about a repairman who was in t...       1\n",
       "3  The Johnny Wander autobio omnibus you've all b...       1\n",
       "4  The vision for this project is the establishme...       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = pd.read_csv(\"kickstarter-data-modified.csv\")\n",
    "ks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tokenize everything: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blurbs</th>\n",
       "      <th>Backed</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Catalysts, Explorers &amp; Secret Keepers: Women ...</td>\n",
       "      <td>1</td>\n",
       "      <td>'Catalysts, Explorers &amp; Secret Keepers: Women ...</td>\n",
       "      <td>[Catalysts, Explorers, Secret, Keepers, Women,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A unique handmade picture book for kids &amp; art ...</td>\n",
       "      <td>0</td>\n",
       "      <td>A unique handmade picture book for kids &amp; art ...</td>\n",
       "      <td>[A, unique, handmade, picture, book, for, kids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A horror comedy about a repairman who was in t...</td>\n",
       "      <td>1</td>\n",
       "      <td>A horror comedy about a repairman who was in t...</td>\n",
       "      <td>[A, horror, comedy, about, a, repairman, who, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Johnny Wander autobio omnibus you've all b...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Johnny Wander autobio omnibus you've all b...</td>\n",
       "      <td>[The, Johnny, Wander, autobio, omnibus, you, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The vision for this project is the establishme...</td>\n",
       "      <td>0</td>\n",
       "      <td>The vision for this project is the establishme...</td>\n",
       "      <td>[The, vision, for, this, project, is, the, est...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Blurbs  Backed  \\\n",
       "0  'Catalysts, Explorers & Secret Keepers: Women ...       1   \n",
       "1  A unique handmade picture book for kids & art ...       0   \n",
       "2  A horror comedy about a repairman who was in t...       1   \n",
       "3  The Johnny Wander autobio omnibus you've all b...       1   \n",
       "4  The vision for this project is the establishme...       0   \n",
       "\n",
       "                                               clean  \\\n",
       "0  'Catalysts, Explorers & Secret Keepers: Women ...   \n",
       "1  A unique handmade picture book for kids & art ...   \n",
       "2  A horror comedy about a repairman who was in t...   \n",
       "3  The Johnny Wander autobio omnibus you've all b...   \n",
       "4  The vision for this project is the establishme...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Catalysts, Explorers, Secret, Keepers, Women,...  \n",
       "1  [A, unique, handmade, picture, book, for, kids...  \n",
       "2  [A, horror, comedy, about, a, repairman, who, ...  \n",
       "3  [The, Johnny, Wander, autobio, omnibus, you, v...  \n",
       "4  [The, vision, for, this, project, is, the, est...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = ks\n",
    "\n",
    "t = time()\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['Blurbs'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "\n",
    "print('Time to tokenize everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "backedData = df_clean[df_clean['Backed'] == 1]\n",
    "unbackedData = df_clean[df_clean['Backed'] == 0]\n",
    "backedTokens = backedData['tokens'].tolist()\n",
    "backedWords = []\n",
    "for token in backedTokens:\n",
    "    backedWords += token\n",
    "unbackedTokens = unbackedData['tokens'].tolist()\n",
    "unbackedWords = []\n",
    "for token in unbackedTokens:\n",
    "    unbackedWords += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n",
      "Time to train the model: 0.04 mins\n"
     ]
    }
   ],
   "source": [
    "#WORD2VEC()\n",
    "#removed size\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer, important for a parameter of the model\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time()\n",
    "w2v_model.build_vocab(df_clean[\"tokens\"], progress_per=1000)\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(df_clean[\"tokens\"], total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First defining the X (input), and the y (output)\n",
    "y = df_clean['Backed'].values\n",
    "X = np.array(df_clean[\"tokens\"])\n",
    "\n",
    "#And here is the train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 800\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        #print(word)\n",
    "        try:\n",
    "            vec += w2v_model.wv[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-857aafb5f091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'The'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.vector_size\n",
    "w2v_model['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=471, vector_size=100, alpha=0.03)\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"good\"])\n",
    "print(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (3200, 100) \n",
      "shape for test set :  (800, 100)\n"
     ]
    }
   ],
   "source": [
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 100) for z in map(lambda x: x, X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 100) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 128)               12928     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,057\n",
      "Trainable params: 13,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 1s 4ms/step - loss: 0.7782 - accuracy: 0.6125 - val_loss: 0.6260 - val_accuracy: 0.7100\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.8030 - accuracy: 0.6328 - val_loss: 0.6242 - val_accuracy: 0.7100\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7699 - accuracy: 0.6153 - val_loss: 0.6227 - val_accuracy: 0.7150\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7902 - accuracy: 0.6303 - val_loss: 0.6214 - val_accuracy: 0.7175\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7939 - accuracy: 0.6134 - val_loss: 0.6199 - val_accuracy: 0.7225\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.8093 - accuracy: 0.6250 - val_loss: 0.6186 - val_accuracy: 0.7225\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7697 - accuracy: 0.6356 - val_loss: 0.6174 - val_accuracy: 0.7225\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7825 - accuracy: 0.6256 - val_loss: 0.6162 - val_accuracy: 0.7275\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7634 - accuracy: 0.6228 - val_loss: 0.6150 - val_accuracy: 0.7300\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7792 - accuracy: 0.6325 - val_loss: 0.6138 - val_accuracy: 0.7312\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7831 - accuracy: 0.6434 - val_loss: 0.6125 - val_accuracy: 0.7350\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7910 - accuracy: 0.6259 - val_loss: 0.6113 - val_accuracy: 0.7387\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7663 - accuracy: 0.6347 - val_loss: 0.6101 - val_accuracy: 0.7387\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 0.6309 - val_loss: 0.6091 - val_accuracy: 0.7387\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7676 - accuracy: 0.6388 - val_loss: 0.6079 - val_accuracy: 0.7412\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7788 - accuracy: 0.6491 - val_loss: 0.6067 - val_accuracy: 0.7412\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7564 - accuracy: 0.6284 - val_loss: 0.6056 - val_accuracy: 0.7412\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7444 - accuracy: 0.6356 - val_loss: 0.6046 - val_accuracy: 0.7425\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7203 - accuracy: 0.6381 - val_loss: 0.6035 - val_accuracy: 0.7425\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7510 - accuracy: 0.6381 - val_loss: 0.6024 - val_accuracy: 0.7462\n",
      "Training Accuracy: 0.7228\n",
      "Testing Accuracy:  0.7462\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_vecs_w2v, y_train, epochs=20, batch_size=50,\n",
    "                   validation_data=(test_vecs_w2v,y_test))\n",
    "loss, accuracy = model.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "ks = pd.read_csv(\"kickstarter-data-modified.csv\")\n",
    "ks.head()\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "ksBlurbs = ks['Blurbs'].tolist()\n",
    "ksBacked = ks['Backed'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(ksBlurbs, ksBacked, test_size = 0.2, random_state = 0)\n",
    "model.fit(X_train, y_train)\n",
    "labels = model.predict(X_test)\n",
    "correct = 0\n",
    "for x in range(0,len(labels)):\n",
    "    if labels[x] == y_test[x]:\n",
    "        correct += 1\n",
    "accuracy = correct / len(labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Choose an NLP problem: Classification (Predict success on Kickstarter based on Kickstarter summary) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Identify or construct a solution based on a generative probabilistic (language) model. Describe the model in detail and develop a solution using parameter inference (and/or decoding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split data 25/75\n",
    "- Gaussian Naive Bayes is implemented \n",
    "- training data is fit to model \n",
    "- predictions\n",
    "- confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Identify or construct a solution based on a (discriminative) neural network. Describe the network structure in detail and develop a solution using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our neural network approach, first we take the cleaned data and then using googles news word to vector model we can convert our Kickstarter blurbs into vectors as well split into test and training. Then we feed that into our sequential neural network with the following layers a relu activation function followed by a dropout layer of 0.7 and then a sigmoid layer with a binary cross entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Apply both approaches to synthetic data that you generate according to the generative model from Step 2. Evaluate the results qualitatively and quantitatively. Highlight situations where each approach performs well and poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Apply both approaches to “real” data acquired legally. Evaluate the results qualitatively and quantitatively. Highlight situations where each approach performs well and poorly. Any unusual/unexpected results require explanation (and frankly, probably debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Discuss pros and cons of the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
